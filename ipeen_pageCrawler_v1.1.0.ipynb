{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests as r\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import json \n",
    "from queue import Queue\n",
    "import threading\n",
    "import re\n",
    "import time\n",
    "url_base = 'http://www.ipeen.com.tw'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最上層爬蟲"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#爬取愛評網各個縣市分類的餐廳評論頁數網址\n",
    "def page_crawler():\n",
    "    startTime = time.time()\n",
    "    #台灣餐廳評論的所有頁數從最新到最舊\n",
    "    try:\n",
    "        url = 'http://www.ipeen.com.tw/search/taiwan/000/1-0-0-0/'\n",
    "        res = r.get(url)\n",
    "        res.encoding=\"utf-8\"\n",
    "        soup = bs(res.text, 'lxml')\n",
    "        page_url_list=[]\n",
    "        #取得最後一頁的頁數\n",
    "        finalpage = soup.select('.next_p_s > a')[0].get('href').split('=')[1]\n",
    "        int_page = int(finalpage)\n",
    "        \n",
    "        f1 = open('all_pages_list_block1.txt', 'a',encoding='utf8')\n",
    "        f2 = open('all_pages_list_block2.txt', 'a',encoding='utf8')\n",
    "        f3 = open('all_pages_list_block3.txt', 'a',encoding='utf8')\n",
    "        f4 = open('all_pages_list_block4.txt', 'a',encoding='utf8')\n",
    "        \n",
    "        try:\n",
    "            for page_number in range(1,int_page+1):\n",
    "                page_url = url+'?p={}'.format(page_number)\n",
    "\n",
    "                #print(page_url)\n",
    "                if page_number%4==1:\n",
    "                    f1.write(page_url+'\\n')\n",
    "\n",
    "                elif page_number%4==2:\n",
    "                    f2.write(page_url+'\\n')\n",
    "                elif page_number%4==3:        \n",
    "                    f3.write(page_url+'\\n')\n",
    "                else:     \n",
    "                    f4.write(page_url+'\\n')\n",
    "        except:\n",
    "            pass\n",
    "            print('[ERROR]IOexception!')\n",
    "        finally:\n",
    "            f1.close()\n",
    "            f2.close()\n",
    "            f3.close()\n",
    "            f4.close()\n",
    "        \n",
    "            #q.put(page_url)\n",
    "            #呼叫取得餐廳裡的所有評論網址的方法\n",
    "            #all_restaurant_list(page_url)\n",
    "        endTime = time.time()\n",
    "        print(endTime-startTime)\n",
    "    #避免遇到index error\n",
    "    except IndexError:\n",
    "        print(\"[ERROR] : index error!\")\n",
    "    #爬取所有餐廳list，並看該餐廳最大的評論數目"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8530488014221191\n"
     ]
    }
   ],
   "source": [
    "page_crawler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url_base_string = 'http://www.ipeen.com.tw/search/taiwan/000/1-0-0-0/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-14-f2d28e41a1d5>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-14-f2d28e41a1d5>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    switcher = {\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def mapping_function(number, split_number, url_base_string):\n",
    "\n",
    "    split_block = number % split_number\n",
    "    with open('mapping_split_block_0.txt', 'a',encoding='utf8') as f1:\n",
    "    \n",
    "    \n",
    "    switcher = {\n",
    "        0: lambda: f1.write(url_base_string+'?p={}'.format(number)+'\\n'),\n",
    "        1: lambda: with open('mapping_split_block_1.txt', 'a',encoding='utf8') as f2: f2.write(url_base_string+'?p={}'.format(number)+'\\n'),\n",
    "        2: lambda: with open('mapping_split_block_2.txt', 'a',encoding='utf8') as f3: f3.write(url_base_string+'?p={}'.format(number)+'\\n'),\n",
    "        3: lambda: with open('mapping_split_block_3.txt', 'a',encoding='utf8') as f4: f4.write(url_base_string+'?p={}'.format(number)+'\\n'),\n",
    "        4: lambda: with open('mapping_split_block_4.txt', 'a',encoding='utf8') as f5: f5.write(url_base_string+'?p={}'.format(number)+'\\n'),\n",
    "        5: lambda: with open('mapping_split_block_5.txt', 'a',encoding='utf8') as f6: f6.write(url_base_string+'?p={}'.format(number)+'\\n'),\n",
    "        6: lambda: with open('mapping_split_block_6.txt', 'a',encoding='utf8') as f7: f7.write(url_base_string+'?p={}'.format(number)+'\\n'),\n",
    "        7: lambda: with open('mapping_split_block_7.txt', 'a',encoding='utf8') as f8: f8.write(url_base_string+'?p={}'.format(number)+'\\n'),\n",
    "        8: lambda: with open('mapping_split_block_8.txt', 'a',encoding='utf8') as f9: f9.write(url_base_string+'?p={}'.format(number)+'\\n'),\n",
    "        9: lambda: with open('mapping_split_block_9.txt', 'a',encoding='utf8') as f10: f10.write(url_base_string+'?p={}'.format(number)+'\\n'),\n",
    "        }.get(split_block, lambda: print('too many split number!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#爬取單一餐廳的所有食記網址\n",
    "def restaurant_share(share_url):\n",
    "    \n",
    "    \n",
    "    res = r.get(share_url)\n",
    "    res.encoding=\"utf-8\"\n",
    "    soup = bs(res.text, 'lxml')\n",
    "\n",
    "    \n",
    "    comment_list = soup.find_all('a',{'itemprop':'discussionUrl url'})\n",
    "    for comment in comment_list:\n",
    "        comment_url = comment.get('href')\n",
    "        c_url = url_base + comment_url\n",
    "#         print(c_url)\n",
    "        comment_page = comment.get('href').split('/')[2]\n",
    "        int_comment_page = int(comment_page)\n",
    "        \n",
    "        if int_comment_page%4==1:\n",
    "            with open('text_1.txt', 'a',encoding='utf8') as f1:\n",
    "                f1.write(c_url+'\\n')\n",
    "                \n",
    "        elif int_comment_page%4==2:\n",
    "            with open('text_2.txt', 'a',encoding='utf8') as f2:\n",
    "                f2.write(c_url+'\\n')\n",
    "        elif int_comment_page%4==3:        \n",
    "            with open('text_3.txt', 'a',encoding='utf8') as f3:\n",
    "                f3.write(c_url+'\\n')\n",
    "        else:        \n",
    "            with open('text_4.txt', 'a',encoding='utf8') as f4:\n",
    "                f4.write(c_url+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
