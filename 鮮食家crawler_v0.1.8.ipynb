{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests as rq\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from collections import OrderedDict as od\n",
    "from threading import Thread\n",
    "from queuelib import queue\n",
    "from selenium import webdriver\n",
    "from datetime import datetime\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "import csv\n",
    "import traceback as tb\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HOST = 'https://www.famicloud.com.tw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index_url = HOST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 上層爬蟲"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def famicloud_crawler():\n",
    "    res_index = rq.get(index_url)\n",
    "    soup_index = bs(res_index.text, 'lxml')\n",
    "    soup_li = soup_index.li\n",
    "    \n",
    "    \n",
    "    for i in range (1 ,1000):\n",
    "        try:\n",
    "            small_branch = soup_li.select('a')[i].text\n",
    "            pre_small_branch_url = soup_li.select('a')[i]['href']\n",
    "            small_branch_url = \"{}{}\".format(index_url, pre_small_branch_url)\n",
    "            \n",
    "            print(small_branch_url)\n",
    "            #交給中層爬蟲\n",
    "            #pass\n",
    "            small_branch_crawler(small_branch_url)\n",
    "\n",
    "        except KeyError:       \n",
    "            sub_branch = soup_li.select('a')[i].text\n",
    "            print(sub_branch)\n",
    "\n",
    "        except IndexError:\n",
    "            break   \n",
    "\n",
    "    print(\"[INFO] famicloud_crawler complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 中層爬蟲"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def small_branch_crawler(small_branch_url):\n",
    "    # 擷取 JavaScript 渲染的網頁原始碼，先指定 PhantomJs 安裝位置\n",
    "    driver = webdriver.PhantomJS(executable_path='C:/Users/nick800608/phantomjs-2.1.1-windows/bin/phantomjs')\n",
    "     # 輸入範例網址，交給瀏覽器 \n",
    "    driver.get(small_branch_url)\n",
    "    # 取得網頁原始碼 (含 JavaScript)\n",
    "    pageSource = driver.page_source    \n",
    "    # print(pageSource)\n",
    "    soup = bs(pageSource, 'lxml')\n",
    "    \n",
    "    a_href_list = soup.select('#SmallMedium > a')\n",
    "    for a_href in a_href_list:\n",
    "        if a_href['href'] != 'javascript:void(0)':\n",
    "            pre_product_url = a_href['href']\n",
    "            product_url = \"{}{}\".format(HOST, pre_product_url)\n",
    "            print(product_url)\n",
    "            product_page_crawler(product_url)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 下層爬蟲"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def product_page_crawler(product_url):\n",
    "    res = rq.get(product_url)\n",
    "    soup = bs(res.text, 'lxml')\n",
    "    \n",
    "    product_dict = od()\n",
    "    food_info_dict = od()  \n",
    "    \n",
    "    food_name = ''\n",
    "    \n",
    "    #找到最後一個有strong標籤的div，就會是內容\n",
    "    div_index_needed = 0\n",
    "    for div_index in range(1, 70):\n",
    "        if hasattr(soup.select(\"div\")[div_index].strong, 'text'):\n",
    "            if div_index > div_index_needed:\n",
    "                div_index_needed = div_index\n",
    "            #print(div_index_needed)\n",
    "    #print(div_index_needed)\n",
    "    \n",
    "    try:\n",
    "        soup.select(\"div\")[div_index_needed].strong.text\n",
    "        content = soup.select(\"div\")[div_index_needed]\n",
    "        new_content = content\n",
    "        \n",
    "        while(new_content.strong != None):\n",
    "            \n",
    "            try:\n",
    "                food_name += new_content.strong.text\n",
    "                new_content.strong.extract()\n",
    "                \n",
    "            except AttributeError:\n",
    "                break\n",
    "                \n",
    "        food_info_dict[\"food_name\"] = food_name\n",
    "        #print(food_info_dict)\n",
    "        \n",
    "    #------以下應該都沒用了-----\n",
    "    except AttributeError:\n",
    "        soup.select(\"div\")[54].strong.text\n",
    "        content = soup.select(\"div\")[54]\n",
    "        new_content = content\n",
    "        \n",
    "        while(new_content.strong != None):\n",
    "            \n",
    "            try:   \n",
    "                food_name += new_content.strong.text\n",
    "                new_content.strong.extract()\n",
    "                \n",
    "            except AttributeError:\n",
    "                break\n",
    "                \n",
    "        food_info_dict[\"food_name\"] = food_name\n",
    "    #------以上應該都沒用了-----    \n",
    "    \n",
    "    except:\n",
    "        print(\"[***ERROR***]can not find food_name!\")\n",
    "        \n",
    "    while(True):\n",
    "        \n",
    "        try:\n",
    "            new_content.br.replace_with(' ')\n",
    "            \n",
    "        except AttributeError:\n",
    "            break\n",
    "   \n",
    "    food_info_and_other = new_content.text.strip().split() \n",
    "    \n",
    "    #original_price\n",
    "    food_info_dict[\"original_price\"] = soup.select(\".BOriginalPrice\")[0].text\n",
    "    #discount_price\n",
    "    food_info_dict[\"discount_price\"] = soup.select(\".OnSalePrice > b\")[0].text\n",
    "    #stored_method\n",
    "    food_info_dict[\"stored_method\"] = soup.select(\".BProductLabel\")[0].img['alt']\n",
    "    #category\n",
    "    category_list = []\n",
    "    a_hrefs = soup.select('#map-list')[0]\n",
    "    branch_levels = a_hrefs.select('a')\n",
    "    for branch in branch_levels:\n",
    "        if(branch.text != '首頁'):\n",
    "            category_list.append(branch.text)\n",
    "    food_info_dict[\"category\"] = category_list\n",
    "    #weight\n",
    "    food_info_dict[\"weight\"] = food_info_and_other[1]\n",
    "    #expired_date\n",
    "    food_info_dict[\"expired_date\"] = food_info_and_other[7]\n",
    "    #country_of_origin\n",
    "    food_info_dict[\"country_of_origin\"] = food_info_and_other[5]\n",
    "    \n",
    "    #把food_info字典放入product_dict字典\n",
    "    product_dict[\"food_info\"] = food_info_dict\n",
    "    \n",
    "    #steps(未完成)\n",
    "    #product_dict[\"steps\"] = null\n",
    "    \n",
    "    #product_img_url\n",
    "    product_dict[\"product_img_url\"] = soup.select(\".product_img_box\")[0].img['src']\n",
    "    #product_characteristic_img_url\n",
    "    \n",
    "    try:\n",
    "        #不在P標籤內\n",
    "        product_dict[\"product_characteristic_img_url\"] = soup.select('div > img')[1]['src']\n",
    "        #rint(product_dict)\n",
    "        \n",
    "    except IndexError:\n",
    "        #在P標籤內\n",
    "        product_dict[\"product_characteristic_img_url\"] = soup.select(\"div > p > img\")[0]['src']\n",
    "    \n",
    "    #web_url\n",
    "    product_dict[\"web_url\"] = product_url\n",
    "    \n",
    "    print(product_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "famicloud_crawler()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
